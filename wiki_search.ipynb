{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower().strip()\n",
    "        cleaned_text = text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "        return cleaned_text\n",
    "    elif isinstance(text, list):\n",
    "        cleaned_texts = []\n",
    "        for txt in text:\n",
    "            txt = txt.lower().strip()\n",
    "            txt = txt.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "            cleaned_texts.append(txt)\n",
    "        return cleaned_texts\n",
    "    return None\n",
    "\n",
    "def load_truthfulQA():\n",
    "    dataset = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
    "    df = pd.DataFrame(dataset['validation'])\n",
    "    df['question'] = df['question'].apply(clean_text)\n",
    "    df['source'] = df['source'].apply(clean_text)\n",
    "    df['best_answer'] = df['best_answer'].apply(clean_text)\n",
    "    df['correct_answers'] = df['correct_answers'].apply(clean_text)\n",
    "    df['incorrect_answers'] = df['incorrect_answers'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "def load_HaluEval():\n",
    "    dataset = load_dataset(\"pminervini/HaluEval\", \"qa\")\n",
    "    df = pd.DataFrame(dataset['data'])\n",
    "    df['knowledge'] = df['knowledge'].apply(clean_text)\n",
    "    df['question'] = df['question'].apply(clean_text)\n",
    "    df['answer'] = df['answer'].apply(clean_text)\n",
    "    df['hallucination'] = df['hallucination'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "def load_fever():\n",
    "    dataset = load_dataset(\"fever\", \"v1.0\")\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "    print(df.keys())\n",
    "    df['id'] = df['id']\n",
    "    df['claim'] = df['claim'].apply(clean_text)\n",
    "    df['evidence_id'] = df['evidence_id']\n",
    "    df['evidence_wiki_url'] = df['evidence_wiki_url'].apply(clean_text)\n",
    "    df['label'] = df['label'].apply(clean_text)\n",
    "    df['evidence_sentence_id'] = df['evidence_sentence_id']\n",
    "    df['evidence_annotation_id'] = df['evidence_annotation_id']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki search for article links/titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_search(url, no_of_links=5):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        \n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        result_divs = soup.find_all('div', class_='mw-search-result-heading')\n",
    "        search_results = []\n",
    "        \n",
    "        for div in result_divs:\n",
    "            \n",
    "            a_tag = div.find('a')\n",
    "            if a_tag:\n",
    "                link = a_tag.get('href', '')\n",
    "                link.replace(' ', '_')\n",
    "                if link.startswith('/'):\n",
    "                    link = f\"https://en.wikipedia.org{link}\"\n",
    "                \n",
    "                title = a_tag.get('title', '')\n",
    "                \n",
    "                search_results.append({\n",
    "                    'link': link,\n",
    "                    'title': title\n",
    "                })\n",
    "        \n",
    "        return search_results[:no_of_links]\n",
    "    \n",
    "    except:\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Wikipedia content using url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_content(title, url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        content_div = soup.find('div', class_='mw-content-ltr mw-parser-output')\n",
    "        if not content_div:\n",
    "            print(f\"{title}: Content div not found!\")\n",
    "            return None\n",
    "        \n",
    "        content = []\n",
    "        current_heading = None\n",
    "        current_topic = None\n",
    "        break_tags = ['References', 'Sources', 'External links', 'See also', 'Further reading', 'Notes']\n",
    "        \n",
    "        # Process content before TOC\n",
    "        for element in content_div.children:\n",
    "            if not hasattr(element, 'name'):\n",
    "                continue\n",
    "                \n",
    "            # Stop when we find the TOC meta tag\n",
    "            if element.name == 'meta' and element.get('property') == 'mw:PageProp/toc':\n",
    "                break\n",
    "                \n",
    "            # Process content before TOC\n",
    "            if element.name == 'div' and element.get('class'):\n",
    "                if 'mw-heading2' in element.get('class'):\n",
    "                    current_heading = element.get_text()\n",
    "                    content.append(f\"\\n## {current_heading}\\n\")\n",
    "                    current_topic = None\n",
    "                elif 'mw-heading3' in element.get('class'):\n",
    "                    current_topic = element.get_text()\n",
    "                    content.append(f\"\\n### {current_topic}\\n\")\n",
    "                else:\n",
    "                    text = element.get_text()\n",
    "                    if text:\n",
    "                        content.append(text + \"\\n\")\n",
    "            elif element.name == 'p':\n",
    "                text = element.get_text()\n",
    "                if text:\n",
    "                    content.append(text + \"\\n\")\n",
    "            elif element.name == 'ul':\n",
    "                list_items = element.find_all('li')\n",
    "                for i, li in enumerate(list_items, 1):\n",
    "                    text = li.get_text()\n",
    "                    if text:\n",
    "                        content.append(f\"{i}. {text}\\n\")\n",
    "        \n",
    "        # Process content after TOC\n",
    "        toc_meta = content_div.find('meta', property='mw:PageProp/toc')\n",
    "        if toc_meta:\n",
    "            current_element = toc_meta.find_next()\n",
    "            while current_element:\n",
    "                if not hasattr(current_element, 'name'):\n",
    "                    current_element = current_element.find_next()\n",
    "                    continue\n",
    "                    \n",
    "                # Check for break tags\n",
    "                if (current_element.name == 'div' and \n",
    "                    current_element.get('class') and \n",
    "                    ('mw-heading2' in current_element.get('class') or\n",
    "                    'mw-heading3' in current_element.get('class')) and\n",
    "                    any(current_element.get_text().startswith(break_tag) for break_tag in break_tags)):\n",
    "                    break\n",
    "                    \n",
    "                # Process headings\n",
    "                if current_element.name == 'div' and current_element.get('class'):\n",
    "                    if 'mw-heading2' in current_element.get('class'):\n",
    "                        current_heading = current_element.get_text()\n",
    "                        content.append(f\"\\n## {current_heading}\\n\")\n",
    "                        current_topic = None\n",
    "                    elif 'mw-heading3' in current_element.get('class'):\n",
    "                        current_topic = current_element.get_text()\n",
    "                        content.append(f\"\\n### {current_topic}\\n\")\n",
    "                    else:\n",
    "                        text = current_element.get_text()\n",
    "                        if text:\n",
    "                            content.append(text + \"\\n\")\n",
    "                            \n",
    "                # Process paragraphs\n",
    "                elif current_element.name == 'p':\n",
    "                    text = current_element.get_text()\n",
    "                    if text:\n",
    "                        content.append(text + \"\\n\")\n",
    "                        \n",
    "                # Process lists\n",
    "                elif current_element.name == 'ul':\n",
    "                    list_items = current_element.find_all('li')\n",
    "                    for i, li in enumerate(list_items, 1):\n",
    "                        text = li.get_text()\n",
    "                        if text:\n",
    "                            content.append(f\"{i}. {text}\\n\")\n",
    "                \n",
    "                current_element = current_element.find_next()\n",
    "        \n",
    "        result = ''.join(content)\n",
    "        result = result.replace('[edit]', '')\n",
    "        results = result.split('\\n')\n",
    "        final_results = []\n",
    "        for line in results:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            if line.startswith('###'):\n",
    "                line = f\"\\n{line}\"\n",
    "            elif line.startswith('##'):\n",
    "                line = f\"\\n{line}\\n\"\n",
    "            elif line.startswith('#'):\n",
    "                line = f\"{line}\\n\\n\"\n",
    "            if line not in final_results:\n",
    "                final_results.append(line)\n",
    "            \n",
    "        result = '\\n'.join(final_results)\n",
    "        result = \"# \" + title + \"\\n\\n\" + result\n",
    "        return result\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Load Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_json(dictionary, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dictionary, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Dictionary saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving dictionary: {e}\")\n",
    "\n",
    "def load_dict_from_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dictionary: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truthful QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used LLM to extract domain for each question and saved in a csv file along with entire dataset\n",
    "# Load dataset with domains\n",
    "\n",
    "df = pd.read_csv('truthfulQA_domain.csv')\n",
    "\n",
    "# Find unique domains\n",
    "domains = []\n",
    "for index, row in df.iterrows():\n",
    "    dm = row['domain']\n",
    "    domains.append(dm)\n",
    "    \n",
    "unique_domains = list(set(domains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get wiki articles for domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_pages = {}\n",
    "for dm in unique_domains:\n",
    "    url = f\"https://en.wikipedia.org/w/index.php?search={'+'.join(dm.split())}&title=Special%3ASearch&profile=advanced&fulltext=1&ns0=1\"\n",
    "    domain_pages[dm] = get_wikipedia_search(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract wikipedia article content for each domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, page in enumerate(domain_pages, 1):\n",
    "    for j, article in enumerate(domain_pages[page], 1):\n",
    "        print(f'Processing: {i}-{j}')\n",
    "        article['doc'] = extract_wikipedia_content(article['title'], article['link'])\n",
    "        if article['doc'] == None:\n",
    "            print(article['title'], \": Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save documents into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_json(domain_pages, 'truthfulqa_domain_docs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HaluEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_HaluEval()\n",
    "\n",
    "# extract knowledge for each \n",
    "halueval_knowledge = []\n",
    "for index, row in df.iterrows():\n",
    "    knw = str(row['knowledge']).lower()\n",
    "    if knw != \"nan\":\n",
    "        halueval_knowledge.append({'id': index, 'knowledge': knw})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save documents into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_json(halueval_knowledge, 'halueval_knowledge.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_fever()\n",
    "\n",
    "# Filter unique evidence wikipedia urls\n",
    "unique_wiki = set()\n",
    "for index, row in df.iterrows():\n",
    "    src = str(row['evidence_wiki_url']).strip()\n",
    "    if src != '' and src != 'nan':\n",
    "        unique_wiki.add(src)\n",
    "unique_wiki = list(unique_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special characters for wikipedia search\n",
    "special_chars_encoding = {\n",
    "    ' ': '%20',\n",
    "    '!': '%21',\n",
    "    '\"': '%22',\n",
    "    '#': '%23',\n",
    "    '$': '%24',\n",
    "    '%': '%25',\n",
    "    '&': '%26',\n",
    "    \"'\": '%27',\n",
    "    '(': '%28',\n",
    "    ')': '%29',\n",
    "    '*': '%2A',\n",
    "    '+': '%2B',\n",
    "    ',': '%2C',\n",
    "    '/': '%2F',\n",
    "    ':': '%3A',\n",
    "    ';': '%3B',\n",
    "    '<': '%3C',\n",
    "    '=': '%3D',\n",
    "    '>': '%3E',\n",
    "    '?': '%3F',\n",
    "    '@': '%40',\n",
    "    '[': '%5B',\n",
    "    '\\\\': '%5C',\n",
    "    ']': '%5D',\n",
    "    '^': '%5E',\n",
    "    '`': '%60',\n",
    "    '{': '%7B',\n",
    "    '|': '%7C',\n",
    "    '}': '%7D',\n",
    "    '~': '%7E'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get wiki article for each evidence url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fever_wiki_pages = {}\n",
    "for article in unique_wiki:\n",
    "    article_new = ''.join([article])\n",
    "    for key in special_chars_encoding:\n",
    "        article_new = article_new.replace(key, special_chars_encoding[key])\n",
    "    url = f\"https://en.wikipedia.org/w/index.php?search={article_new}&title=Special%3ASearch&profile=advanced&fulltext=1&ns0=1\"\n",
    "    fever_wiki_pages[article] = get_wikipedia_search(url, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract wikipedia article content for each evidence url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, page in enumerate(fever_wiki_pages, 1):\n",
    "    for j, article in enumerate(fever_wiki_pages[page], 1):\n",
    "        print(f'Processing: {i}-{j}')\n",
    "        article['doc'] = extract_wikipedia_content(article['title'], article['link'])\n",
    "        if article['doc'] == None:\n",
    "            print(article['title'], \": Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save documents into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_json(fever_wiki_pages, 'fever_wiki_pages.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
